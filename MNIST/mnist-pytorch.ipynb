{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning basic (PyTorch)\n",
    "\n",
    "- **Instructor**: Jongwoo Lim / Jiun Bae\n",
    "- **Email**: [jlim@hanyang.ac.kr](mailto:jlim@hanyang.ac.kr) / [jiun.maydev@gmail.com](mailto:jiun.maydev@gmail.com)\n",
    "\n",
    "## MNIST Example\n",
    "\n",
    "In this example you will practice a simple neural network written by [PyTorch](https://pytorch.org), using the basically used handwritten digits data set, [MNIST Dataset](http://yann.lecun.com/exdb/mnist). The goals of this example are as follows:\n",
    "\n",
    "- Learn basically how to **write and use code**(*PyTorch*).\n",
    "- Understand **Neural Networks** and how they work.\n",
    "\n",
    "*If you are more familiar with TensorFlow(or Keras), We'll see if We can help you in other ways. But PyTorch is still a good choice for beginners(or expert also).*\n",
    "\n",
    "And this example also is written in [IPython Notebook](https://ipython.org/notebook.html), an interactive computational environment, in which you can run code directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments\n",
    "\n",
    "In this assignment, we assume the follows environments. \n",
    "The [Python](https://www.python.org) is a programming language that lets you work quickly and integrate systems more effectively. It is widely used in various fields, and also used in machine learning.\n",
    "The [Pytorch](https://pytorch.org) is an open source deep learning platform, provides a seamless path from research to production.\n",
    "The [CUDAÂ®](https://developer.nvidia.com/cuda-zone) Toolkit provides high-performance GPU-accelerated computation. In deep learning, the model takes an age to train without GPU-acceleration. ~~even with the GPU, it still takes a lot of time~~.\n",
    "\n",
    "\n",
    "- [Python3](https://www.python.org/downloads/) (recommend 3.6 or above)\n",
    "- [PyTorch](https://pytorch.org) (recommend 1.0)\n",
    "- [NumPy](http://www.numpy.org) the fundamental package for scientific computing with Python\n",
    "\n",
    "\n",
    "- (Optional) [Anaconda](https://www.anaconda.com/distribution/#download-section), *popular Python Data Science Platform*\n",
    "- (Optional) [TensorFlow](https://www.tensorflow.org/install), but examples are provided as PyTorch\n",
    "- (Optional) [Jupyter](https://jupyter.org/) (Notebook or Lab)\n",
    "- (Optional) [CUDA](https://developer.nvidia.com/cuda-downloads) support GPU\n",
    "\n",
    "\n",
    "Python packages can install by `pip install [package name]` or using **Anaconda** by `conda install [package name]`.\n",
    "\n",
    "*If you are having trouble installing or something else, please contact TA or jiun.maydev@gmail.com.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "First of all, Import some packages for using PyTorch.\n",
    "\n",
    "- torch.nn: The **Network** of PyTorch basically starts with nn.Module.\n",
    "- torch.nn.functional: for **Functions** such as *ReLU*, *MaxPool* (in this example)\n",
    "- torch.optim: for **Optimizers**\n",
    "- torchvision: Handling **Datasets**\n",
    "\n",
    "Numpy the basic scientific computing package used in customary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "PyTorch basically provides MNIST Dataset and support download in running code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './data' # path to download mnist dataset\n",
    "\n",
    "TRAIN_DATASET = datasets.MNIST(DATASET_DIR,   # Dataset root path\n",
    "                               train=True,    # Train data\n",
    "                               download=True) # Download if not exist\n",
    "\n",
    "TEST_DATASET = datasets.MNIST(DATASET_DIR,    # Dataset root path\n",
    "                              train=False)    # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Visualize MNIST Dataset\n",
    "\n",
    "This code is just show MNIST data (code is not important!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  # PIL is Python Image Library \n",
    "\n",
    "n = 5                  # 5 x 5 matrix\n",
    "size = 28              # Each image is 28 x 28\n",
    "\n",
    "result = Image.new('L', (size * n, size * n))\n",
    "for i, idx in enumerate(np.random.choice(len(TRAIN_DATASET), n * n)):\n",
    "    x = i // n * size\n",
    "    y = i % n * size\n",
    "    result.paste(TRAIN_DATASET[idx][0], (x, y, x + size, y + size))\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Network\n",
    "\n",
    "This is a simple two convolution layer network. The code is quite easy.\n",
    "Just fallow `forward` function. `torch.nn.functional` and `torch.nn` support many useful function and layer for build network.\n",
    "(See offical [doc](https://pytorch.org/docs/stable/nn.html#torch-nn-functional) for more information)\n",
    "\n",
    "`nn.Conv2d` is 2-dimentional convolutaion layer.\n",
    "\n",
    "`nn.Linear` is linear transformation as `y=ax+b`.\n",
    "\n",
    "`F.relu` apply ReLU activation function.\n",
    "\n",
    "`F.max_pool2d` apply 2-dimentional max-pooling.\n",
    "\n",
    "So, whole network architecture as follow:\n",
    "\n",
    "- Conv1 (input: 1, output: 20, kernel: 5, stride: 1)\n",
    "- ReLU\n",
    "- MaxPool\n",
    "- Conv2 (input: 20, output: 50, kernel: 5, stride: 1)\n",
    "- ReLU\n",
    "- MaxPool\n",
    "- FC1 (input: 4x4x50, output: 500)\n",
    "- ReLU\n",
    "- FC2 (input: 500, output: 10, class number of digit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"Simple Neural Network contains conv layer and fc layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 392)\n",
    "        self.fc2 = nn.Linear(392, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Network\n",
    "\n",
    "This is a simple two convolution layer network. The code is quite easy.\n",
    "Just fallow `forward` function. `torch.nn.functional` and `torch.nn` support many useful function and layer for build network.\n",
    "(See offical [doc](https://pytorch.org/docs/stable/nn.html#torch-nn-functional) for more information)\n",
    "\n",
    "`nn.Conv2d` is 2-dimentional convolutaion layer.\n",
    "\n",
    "`nn.Linear` is linear transformation as `y=ax+b`.\n",
    "\n",
    "`F.relu` apply ReLU activation function.\n",
    "\n",
    "`F.max_pool2d` apply 2-dimentional max-pooling.\n",
    "\n",
    "So, whole network architecture as follow:\n",
    "\n",
    "- Conv1 (input: 1, output: 20, kernel: 5, stride: 1)\n",
    "- ReLU\n",
    "- MaxPool\n",
    "- Conv2 (input: 20, output: 50, kernel: 5, stride: 1)\n",
    "- ReLU\n",
    "- MaxPool\n",
    "- FC1 (input: 4x4x50, output: 500)\n",
    "- ReLU\n",
    "- FC2 (input: 500, output: 10, class number of digit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"Simple Neural Network contains conv layer and fc layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test functions\n",
    "\n",
    "In train scope, we put the data to model, compute loss and update parameters. That's all we have to do.\n",
    "To put data into the model, you call the model as a function that takes data as arguments. (see `output = model(data)`)\n",
    "**PyTorch** provide many loss function. `nll_loss` is the *negative log likelihood loss*. compute loss using output and target(label).\n",
    "Calling `.backward()` accumulates the gradient for each parameters.\n",
    "Finally, call `optimizer.step()` to update optimizer.\n",
    "\n",
    "\n",
    "In test scope, evaluate model using test set which not contained in a train set.\n",
    "`pred = output.argmax(dim=1, keepdim=True)` is change output to digit-class for comparing to target(label).\n",
    "`pred.eq(target.view_as(pred)).sum().item()` calculate correct answer count.\n",
    "\n",
    "\n",
    "`model.train` and `model.test` means set `model` for train or test. In train scope set parameters trainable and test scope not update parameters.\n",
    "So, if you want to update parameters, must call `model.train` before pass data to the model.\n",
    "\n",
    "*`data.to(device)` and `target.to(device)` mean change data to device(gpu if available else cpu)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def train(model, device, train_loader, optimizer) -> float:\n",
    "    \"\"\"Train function\n",
    "\n",
    "        Arguments:\n",
    "            model (nn.Module): some networks extends from ``nn.Module``.\n",
    "            device (torch.device): device for use CUDA if available.\n",
    "            train_loader (torch.utils.data.dataloader.DataLoader): train data loader.\n",
    "            optimizer (torch.optim): optimizer\n",
    "\n",
    "        Returns: loss (float)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model, device, test_loader) -> Tuple[float, float, torch.Tensor]:\n",
    "    \"\"\"Test function\n",
    "\n",
    "        Arguments:\n",
    "            model (nn.Module): some networks extends from ``nn.Module``.\n",
    "            device (torch.device): device for use CUDA if available.\n",
    "            test_loader (torch.utils.data.dataloader.DataLoader): test data loader.\n",
    "\n",
    "        Returns: tuple of below three\n",
    "            test_loss: \n",
    "            accuracy: \n",
    "            output: \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, accuracy, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run!\n",
    "\n",
    "\n",
    "### Reproducible (**Important**)\n",
    "\n",
    "**Reproducible** is very **very** ***very*** important in experiment. An experiment that can not be reproduced can not make any conclusions. So fix random seed before anything else.\n",
    "In **PyTorch** just call `torch.manual_seed` for fix random seed. It will set the seed of the random generator, so random results will be **reproducible**.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Unfortunately, machine learning does not mean learning all the variables. We call these parameters the **hyperparameters** that need to be set before learning.\n",
    "In this example, we can set the *learning rate* before training.\n",
    "\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "Loading files from disk is a very expensive operation. Especially in machine learning where a lot of training data is needed, also especially if each data is an image.\n",
    "So, many frameworks provide *data loader* for effectively load data such as use multiple threads and cache. In **PyTorch** DataLoader support shuffle, batch slice, transform and many other functions.\n",
    "But in this example, just use `batch_size` and `shuffle`.\n",
    "\n",
    "PyTorch only process `torch.Tensor`. So, must convert data (3d numpy array) to tensor (torch.Tensor) using transform before training(or test).\n",
    "*`transforms.ToTensor()` automatically transform data to tensor when loader called*\n",
    "\n",
    "\n",
    "### GPU or CPU\n",
    "\n",
    "`device` variable use **CUDA** if available. CPU can get results fast enough because there are fewer data and the network is simple. But when more data is available and the network gets more complicated, it's time to get help from the GPU. So now you do not have to worry.\n",
    "Later `.to (device)` means use the device we specified.\n",
    "\n",
    "\n",
    "### Build Network\n",
    "\n",
    "`model = Network()` create network we defined before. In this example, we use [SGD(Stochastic gradient descent)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # 42, THE ANSWER TO LIFE, THE UNIVERSE AND EVERYTHING\n",
    "\n",
    "batch = 64            # batch size\n",
    "lr = .01              # learning rate\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "TRAIN_DATASET.transform = transforms.ToTensor()\n",
    "train_loader = torch.utils.data.DataLoader(TRAIN_DATASET,\n",
    "                                           batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "TEST_DATASET.transform = transforms.ToTensor()\n",
    "test_loader = torch.utils.data.DataLoader(TEST_DATASET,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Network().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer)\n",
    "    test_loss, accuracy, _ = test(model, device, test_loader)\n",
    "    \n",
    "    print('Epoch: {}\\t Loss: {:.6f}'.format(epoch, train_loss))\n",
    "    print('\\t\\t Average Loss: {:.4f}, Accuracy: {:.0f}%'.format(test_loss, accuracy))\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")  # save trained model named 'mnist_cnn.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
