{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Example\n",
    "\n",
    "- **Instructor**: Jongwoo Lim / Jiun Bae\n",
    "- **Email**: [jlim@hanyang.ac.kr](mailto:jlim@hanyang.ac.kr) / [jiunbae.623@gmail.com](mailto:jiunbae.623@gmail.com)\n",
    "\n",
    "## Sequential data prediction\n",
    "\n",
    "If you are using a neural network to solve the problem of predicting a sequence, such as sentence or time series data, the size of the vector representing the sequence if you want the values you want to predict depend on older, older data. Should be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (Recurrent Neural Network)\n",
    "\n",
    "RNN (Recurrent Neural Network) is a neural network structure that can predict long sequences by storing the state of a neuron and using it as input in the next step. Here, we will look at the basic structure of RNN and how to implement RNN supported by the Keras Python package.\n",
    "\n",
    "\n",
    "The general feedforward neural network structure is shown as the result of applying the activation function $\\sigma$ to the product of the output vector $y$, the input $x$ vector and the neural network weight matrix $U$ as follows.\n",
    "\n",
    "$$\\sigma(Ux)$$\n",
    "\n",
    "In the case of MLP (Multi-Layer Perceptron) having one hidden layer, it can be expressed as follows:\n",
    "\n",
    "$$h = \\sigma(Ux)$$\n",
    "$$o = \\sigma(Vh)$$\n",
    "\n",
    "In this equation, $h$ is the hidden layer vector, $o$ is the output vector, $U$ is the weight matrix from the input to the hidden layer, and $V$ is the weight matrix from the hidden layer to the output.\n",
    "\n",
    "\n",
    "RNN outputs a status vector $s$ in addition to the output vector $o$. The state vector is similar to a kind of hidden layer vector, but depends on the input vector as well as the previous state vector value. The output vector depends on the value of the state vector.\n",
    "\n",
    "$$s_t = \\sigma(Ux_t + Ws_{t-1})$$\n",
    "$$o_t = \\sigma(Vs_t)$$\n",
    "\n",
    "RNN has a similar effect to MLP, which has an infinite number of hidden layers when connected and unfolded according to time steps. The figure is as follows.\n",
    "\n",
    "![rnn](../assets/rnn.png)\n",
    "\n",
    "The difference is that RNN can process time series data because the state changes from the previous input. The status of the input sequence may change depending on the sequence of the input vectors.\n",
    "\n",
    "It can be divided into the following according to the type of input and result.\n",
    "\n",
    "![sequentail](../assets/sequential.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "$Sin$ function prediction\n",
    "$$y = sin(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.arange(size)\n",
    "y_ = np.sin(np.pi * .125 * x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 30\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.empty((size - input_size, input_size))\n",
    "train_y = np.empty((size - input_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in x_[:-input_size]:\n",
    "    train_x[x] = y_[x:x + input_size]\n",
    "    train_y[x] = y_[x + input_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First train data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_x[0], c='r')\n",
    "plt.scatter(np.arange(input_size), train_x[0], c='b')\n",
    "plt.scatter((input_size,), train_y[0], c='g')\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN Model\n",
    "\n",
    "Network forwards RNN and fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sequential-figure](../assets/sequential-figure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, 1, bias=True, batch_first=True, \n",
    "                          nonlinearity='tanh', dropout=0)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, inputs, states):\n",
    "        out, states = self.rnn(inputs, states)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, states\n",
    "\n",
    "    def state(self):\n",
    "        # return initialized hidden state\n",
    "        return torch.zeros(1, 1, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before training\n",
    "\n",
    "Prediction of un trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = model.state()\n",
    "preds = [\n",
    "    model(torch.Tensor(train_x[x]).view(1, 1, 5).to(device), state)[0].item()\n",
    "    for x in x_[:-input_size]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds, c='b')\n",
    "plt.plot(train_y, c='r')\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # 42, THE ANSWER TO LIFE, THE UNIVERSE AND EVERYTHING\n",
    "\n",
    "batch = 64            # batch size\n",
    "lr = .01              # learning rate\n",
    "epochs = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE loss and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    state = model.state()\n",
    "    loss = 0\n",
    "    \n",
    "    for train, label in zip(train_x, train_y):\n",
    "        train = torch.Tensor(train).view(1, 1, input_size).to(device)\n",
    "        label = torch.Tensor(label).to(device)\n",
    "        out, state = model(train, state)\n",
    "        \n",
    "        loss += criterion(out.squeeze(), label.squeeze())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not (epoch % 8):\n",
    "        model.eval()\n",
    "        print(f'Loss: {loss.item()}')\n",
    "        state = model.state()\n",
    "        preds = [\n",
    "            model(torch.Tensor(train_x[x]).view(1, 1, 5).to(device), state)[0].item()\n",
    "            for x in x_[:-input_size]\n",
    "        ]\n",
    "        \n",
    "        plt.plot(preds, c='b')\n",
    "        plt.plot(train_y, c='r')\n",
    "        plt.xlim(0, 100)\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Change Vanila RNN to LSTM\n",
    "\n",
    "In pytorch, LSTM can be used like this:\n",
    "\n",
    "```\n",
    "nn.LSTM(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, inputs, states):\n",
    "        out, states = self.lstm(inputs, states)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, states\n",
    "    \n",
    "    def state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size).to(device), torch.zeros(1, 1, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Change pattern length\n",
    "\n",
    "how about long long pattern like $y = sin(\\frac{1} {10} x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.arange(size)\n",
    "y_ = np.sin(1/10 * np.pi * .125 * x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Change input_size\n",
    "\n",
    "input_size represents the number of consecutive values received as input. If the pattern length increases, you can adjust the input_size to increase the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.empty((size - input_size, input_size))\n",
    "train_y = np.empty((size - input_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in x_[:-input_size]:\n",
    "    train_x[x] = y_[x:x + input_size]\n",
    "    train_y[x] = y_[x + input_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_x[0], c='r')\n",
    "plt.scatter(np.arange(input_size), train_x[0], c='b')\n",
    "plt.scatter((input_size,), train_y[0], c='g')\n",
    "plt.xlim(0, 48)\n",
    "plt.ylim(-1, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
