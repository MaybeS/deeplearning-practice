{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Tracking\n",
    "\n",
    "- **Instructor**: Jongwoo Lim / Jiun Bae\n",
    "- **Email**: [jlim@hanyang.ac.kr](mailto:jlim@hanyang.ac.kr) / [jiunbae.623@gmail.com](mailto:jiunbae.623@gmail.com)\n",
    "\n",
    "## Object Tracking (MDNet)\n",
    "\n",
    "Object Tracking is tracking object in consecutive image sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem in tracking\n",
    "\n",
    "Visual tracking problems. In traditional approach, tracking using hand-crafted features.\n",
    "\n",
    "![tracking-prablem](../assets/tracking-problem.png)\n",
    "\n",
    "Lack of data for visual tracking. Beacuse image sequences has different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDNet\n",
    "\n",
    "Shared layers and domain-specific layers each domain in trained separately.\n",
    "\n",
    "![MDNet](../assets/MDNet.png)\n",
    "\n",
    "MDNet tactics:\n",
    "\n",
    "1. Bounding box regression\n",
    "2. Hard negative mining\n",
    "3. Consider long-term and short-term changes\n",
    "\n",
    "![MDNet-Regression](../assets/MDNet-regression.png)\n",
    "![MDNet-HardNegativeMining](../assets/MDNet-hardnegative.png)\n",
    "![MDNet-Short-Long-Term](../assets/MDNet-long-short-term.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online tracking (at inference)\n",
    "\n",
    "Drop all domain-specific layers, attach new randomly initilized branch.\n",
    "Update when first frame given.\n",
    "\n",
    "![MDNet-online](../assets/MDNet-inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "First of all, Import some packages for using PyTorch.\n",
    "\n",
    "- torch.nn: The **Network** of PyTorch basically starts with nn.Module.\n",
    "- torch.nn.functional: for **Functions** such as *ReLU*, *MaxPool* (in this example)\n",
    "- torch.optim: for **Optimizers**\n",
    "- torchvision: Handling **Datasets**\n",
    "\n",
    "Numpy the basic scientific computing package used in customary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = '../assets/detection.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model SSD VGG16\n",
    "\n",
    "Single-Shot Multibox Detector (SSD) is a paper published in ECCV in 2016 and predicts the class score and position of the bounding box like other Detection Algorithms. For each input of a single image, bounding box regression and score prediction are performed by using the default box of different ratios and scales in the feature map of the various stages of the features that passed the CNN.\n",
    "\n",
    "![SSD](../assets/SSD.png)\n",
    "\n",
    "The feature extraction convolution filter of several stages added in addition to the backbone generates a fixed number of class scores and bounding box predictions using a small kernel having a $m \\times n$ sized $p$ channel. Includes scores for. Therefore, since there are $k$ cell positions and 4 offset information is calculated for $c$ classes, each cell has $(c+4)\\times k$ filters, and as a result, the feature map has $(c + 4) \\times k \\times m \\times n$ outputs.\n",
    "\n",
    "When training, apply loss function and back propagation like other machine learning networks. In the learning process, you can adjust the number and scale of the default boxes described below and use hard negative mining and data augmentation to improve performance.\n",
    "\n",
    "The total loss function is calculated as the sum of the weights of localization loss (loc) and confidence loss (conf).\n",
    "\n",
    "$$L(x, c, l, g) = \\frac {1} {N} (L_{conf}(x, c) + \\alpha L_{loc} (x, l, g)$$\n",
    "\n",
    "$x^p_{ij} = \\{1, 0\\}$ is i-th default box of klass $p$'s j-th true value box indicator, $N$ is matched default box count.\n",
    "\n",
    "Localization loss using smooth L1 loss between predict box $l$ and ground truth bounding box $g$.\n",
    "\n",
    "$$L_{loc}(x, l, g) = \\sum^N_{i \\in Pos} \\sum_{m \\in \\{ cx, cy, w, h\\} } x^k_{ij} smooth_{L1}(l^m_i - \\hat{g}^m_j)$$\n",
    "\n",
    "Confidence loss is softmax loss of multiple classes.\n",
    "\n",
    "$$L_{conf}(x, c) = - \\sum^N_{i \\in Pos} x^p_{ij} log (\\hat {c}^p_i) - \\sum_{i \\in Neg} log(\\hat {c}^0_i) where \\hat {c}^p_i = \\frac{exp(c^p_i}{\\sum_p exp( c^p_i) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSD.model import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16.new(21, 1).to(device)\n",
    "model.eval()\n",
    "model.load(torch.load('../data/vgg16-pretrained.pth', map_location=lambda s, l: s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSD.lib.augmentation import Compose, ToPercentCoords, Resize, SubtractMeans, ConvertFromInts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    ConvertFromInts(),\n",
    "    ToPercentCoords(),\n",
    "    Resize((300, 300)),\n",
    "    SubtractMeans((123, 117, 104)),\n",
    "    lambda img, boxes=None, labels=None: (img / 1., boxes, labels),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array(image)\n",
    "inputs = cv2.resize(inputs, (300, 300)).astype(np.float32)\n",
    "inputs -= (104, 117, 123)\n",
    "inputs = inputs[:, :, ::-1].copy()\n",
    "inputs = torch.from_numpy(inputs).permute(2, 0, 1)\n",
    "inputs = Variable(inputs.unsqueeze_(0), requires_grad=False)\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 21\n",
    "class_names = ('BACKGROUND',\n",
    "               'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "detection = np.empty((0, 6), dtype=np.float32)\n",
    "\n",
    "for klass, boxes in enumerate(outputs[0]):\n",
    "    candidates = boxes[boxes[:, 0] >= .3]\n",
    "    print(klass, candidates)\n",
    "\n",
    "    if candidates.size(0) == 0:\n",
    "        continue\n",
    "\n",
    "    detection = np.concatenate((\n",
    "        detection,\n",
    "        np.hstack((\n",
    "            np.full((np.size(candidates, 0), 1), klass, dtype=np.uint8),\n",
    "            candidates.cpu().detach().numpy(),\n",
    "        )),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(ary):\n",
    "    display(Image.fromarray(ary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(image)\n",
    "h, w, c = img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255)) for _ in range(21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for klass, conf, x, y, x2, y2 in detection:\n",
    "    if conf < .3:\n",
    "        continue\n",
    "    try:\n",
    "        cv2.rectangle(img, (int(x *w ), int(y *h)), (int(x2 * w), int(y2 * h)), colors[int(klass)], 2)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step in to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass in SSD(VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor) \\\n",
    "            -> Union[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "    \"\"\"Applies network layers and ops on input image(s) x.\n",
    "\n",
    "    Args:\n",
    "        x: input image or batch of images. Shape: [batch,3,300,300].\n",
    "\n",
    "    Return:\n",
    "        Depending on phase:\n",
    "        test:\n",
    "            Variable(tensor) of output class label predictions,\n",
    "            confidence score, and corresponding location predictions for\n",
    "            each object detected. Shape: [batch, topk, 7]\n",
    "\n",
    "        train:\n",
    "            list of concat outputs from:\n",
    "                1: confidence layers, Shape: [batch*num_priors, num_classes]\n",
    "                2: localization layers, Shape: [batch, num_priors*4]\n",
    "                3: priorbox layers, Shape: [2, num_priors*4]\n",
    "    \"\"\"\n",
    "    def _forward(tensor: torch.Tensor, module: nn.Module) \\\n",
    "            -> torch.Tensor:\n",
    "        return module.forward(tensor)\n",
    "\n",
    "    start, sources = 0, []\n",
    "\n",
    "    # forward layers for extract sources\n",
    "    for index, layer, *_ in self.appendix:\n",
    "        x = reduce(_forward, [x, *self.features[start:index]])\n",
    "\n",
    "        if isinstance(layer, GraphPath):\n",
    "            x, y = layer(x, self.features[index])\n",
    "            index += 1\n",
    "\n",
    "        elif layer is not None:\n",
    "            y = layer(x)\n",
    "\n",
    "        else:\n",
    "            y = x\n",
    "\n",
    "        sources.append(y)\n",
    "        start = index\n",
    "\n",
    "    # forward remain parts\n",
    "    x = reduce(_forward, [x, *self.features[start:]])\n",
    "\n",
    "    for i, layer in enumerate(self.extras):\n",
    "        x = _forward(x, layer)\n",
    "        sources.append(x)\n",
    "\n",
    "    def refine(source: torch.Tensor) \\\n",
    "            -> torch.Tensor:\n",
    "        return source.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "    def reshape(tensor: torch.Tensor) \\\n",
    "            -> torch.Tensor:\n",
    "        return torch.cat(tuple(map(lambda t: t.view(t.size(0), -1), tensor)), 1)\n",
    "\n",
    "    locations, confidences = map(reshape, zip(*[(refine(loc(source)), refine(conf(source)))\n",
    "                                                for source, loc, conf in zip(sources, self.loc, self.conf)]))\n",
    "\n",
    "    locations = locations.view(self.batch_size, -1, 4)\n",
    "    confidences = confidences.view(self.batch_size, -1, self.num_classes)\n",
    "\n",
    "    output = (locations, confidences, self.priors.to(x.device))\n",
    "\n",
    "    if not self.training:\n",
    "        output = self.detect(*output).to(x.device)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step forward\n",
    "\n",
    "### First extract feature from inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from SSD.layers import GraphPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward(tensor: torch.Tensor, module: nn.Module) \\\n",
    "        -> torch.Tensor:\n",
    "    return module.forward(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, sources = 0, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward layers for extract sources\n",
    "for index, layer, *_ in model.appendix:\n",
    "    x = reduce(_forward, [x, *model.features[start:index]])\n",
    "\n",
    "    if isinstance(layer, GraphPath):\n",
    "        x, y = layer(x, model.features[index])\n",
    "        index += 1\n",
    "\n",
    "    elif layer is not None:\n",
    "        y = layer(x)\n",
    "\n",
    "    else:\n",
    "        y = x\n",
    "\n",
    "    sources.append(y)\n",
    "    start = index\n",
    "\n",
    "# forward remain parts\n",
    "x = reduce(_forward, [x, *model.features[start:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Calculate output shape of x\n",
    "\n",
    "Input is [1, 3, 300, 300] and pass network like \n",
    "```\n",
    "Sequential(\n",
    "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (1): ReLU(inplace=True)\n",
    "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (3): ReLU(inplace=True)\n",
    "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (6): ReLU(inplace=True)\n",
    "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (8): ReLU(inplace=True)\n",
    "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (11): ReLU(inplace=True)\n",
    "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (13): ReLU(inplace=True)\n",
    "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (15): ReLU(inplace=True)\n",
    "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
    "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (18): ReLU(inplace=True)\n",
    "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (20): ReLU(inplace=True)\n",
    "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (22): ReLU(inplace=True)\n",
    "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (25): ReLU(inplace=True)\n",
    "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (27): ReLU(inplace=True)\n",
    "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (29): ReLU(inplace=True)\n",
    "  (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
    "  (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
    "  (32): ReLU(inplace=True)\n",
    "  (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
    "  (34): ReLU(inplace=True)\n",
    ")\n",
    "```\n",
    "\n",
    "then, what is output shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.extras):\n",
    "    x = _forward(x, layer)\n",
    "    sources.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v.shape for v in sources]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What is in sources?\n",
    "\n",
    "Sources contains below shape of tensor:\n",
    "```\n",
    "[\n",
    "    torch.Size([1, 512, 38, 38]),\n",
    "     torch.Size([1, 1024, 19, 19]),\n",
    "     torch.Size([1, 512, 10, 10]),\n",
    "     torch.Size([1, 256, 5, 5]),\n",
    "     torch.Size([1, 256, 3, 3]),\n",
    "     torch.Size([1, 256, 1, 1])\n",
    "]\n",
    "```\n",
    "\n",
    "What is this and whats for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(source: torch.Tensor) \\\n",
    "        -> torch.Tensor:\n",
    "    return source.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "def reshape(tensor: torch.Tensor) \\\n",
    "        -> torch.Tensor:\n",
    "    return torch.cat(tuple(map(lambda t: t.view(t.size(0), -1), tensor)), 1)\n",
    "\n",
    "locations, confidences = map(reshape, zip(*[(refine(loc(source)), refine(conf(source)))\n",
    "                                            for source, loc, conf in zip(sources, model.loc, model.conf)]))\n",
    "\n",
    "locations = locations.view(model.batch_size, -1, 4)\n",
    "confidences = confidences.view(model.batch_size, -1, model.num_classes)\n",
    "\n",
    "output = (locations, confidences, model.priors.to(x.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Meaning of nubmers\n",
    "\n",
    "locations.shape is [1, 8732, 4] and confidences.shape is [1, 8732, 21].\n",
    "\n",
    "What does each number mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors is default anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors =  model.priors.to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww, hh = 1000, 1000\n",
    "img = np.zeros((ww, hh, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, x2, y2 in priors:\n",
    "    cv2.rectangle(img, (int(x * ww), int(y * hh)), (int(x2 * ww), int(y2 * hh)), (255, 0, 0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "    Objective Loss:\n",
    "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
    "        weighted by α which is set to 1 by cross val.\n",
    "        Args:\n",
    "            c: class confidences,\n",
    "            l: predicted boxes,\n",
    "            g: ground truth boxes\n",
    "            N: number of matched default boxes\n",
    "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, predictions: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: torch.Tensor):\n",
    "        \"\"\"Multibox Loss\n",
    "        Args:\n",
    "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
    "            and prior boxes from SSD net.\n",
    "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
    "                loc shape: torch.size(batch_size,num_priors,4)\n",
    "                priors shape: torch.size(num_priors,4)\n",
    "\n",
    "            targets (tensor): Ground truth boxes and labels for a batch,\n",
    "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
    "        \"\"\"\n",
    "        loc_data, conf_data, priors = predictions\n",
    "        num = loc_data.size(0)\n",
    "        priors = priors[:loc_data.size(1), :]\n",
    "        num_priors = (priors.size(0))\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t, conf_t = torch.Tensor(num, num_priors, 4), torch.LongTensor(num, num_priors)\n",
    "\n",
    "        for idx in range(num):\n",
    "            truths, labels = targets[idx][:, :-1].data, targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "\n",
    "            match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, idx)\n",
    "\n",
    "        loc_t = Variable(loc_t.to(self.device), requires_grad=False)\n",
    "        conf_t = Variable(conf_t.to(self.device), requires_grad=False)\n",
    "\n",
    "        pos = conf_t > 0\n",
    "\n",
    "        # Localization Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,4]\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p, loc_t = loc_data[pos_idx].view(-1, 4), loc_t[pos_idx].view(-1, 4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        loss_c[pos] = 0  # filter out pos boxes for now\n",
    "\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')\n",
    "\n",
    "        N = num_pos.data.sum().double()\n",
    "\n",
    "        loss_l, loss_c = loss_l.double() / N, loss_c.double() / N\n",
    "\n",
    "        return loss_l, loss_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
