{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example\n",
    "\n",
    "- **Instructor**: Jongwoo Lim / Jiun Bae\n",
    "- **Email**: [jlim@hanyang.ac.kr](mailto:jlim@hanyang.ac.kr) / [jiunbae.623@gmail.com](mailto:jiunbae.623@gmail.com)\n",
    "\n",
    "## MNIST dataset\n",
    "\n",
    "In this example you will practice a simple neural network written by [PyTorch](https://pytorch.org), using the basically used handwritten digits data set, [MNIST Dataset](http://yann.lecun.com/exdb/mnist). The goals of this example are as follows:\n",
    "\n",
    "- Learn basically how to **write and use code**(*PyTorch*).\n",
    "- Understand **Neural Networks** and how they work.\n",
    "\n",
    "*If you are more familiar with TensorFlow(or Keras), We'll see if We can help you in other ways. But PyTorch is still a good choice for beginners(or expert also).*\n",
    "\n",
    "And this example also is written in [IPython Notebook](https://ipython.org/notebook.html), an interactive computational environment, in which you can run code directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments\n",
    "\n",
    "In this assignment, we assume the follows environments. \n",
    "The [Python](https://www.python.org) is a programming language that lets you work quickly and integrate systems more effectively. It is widely used in various fields, and also used in machine learning.\n",
    "The [Pytorch](https://pytorch.org) is an open source deep learning platform, provides a seamless path from research to production.\n",
    "The [CUDAÂ®](https://developer.nvidia.com/cuda-zone) Toolkit provides high-performance GPU-accelerated computation. In deep learning, the model takes an age to train without GPU-acceleration. ~~even with the GPU, it still takes a lot of time~~.\n",
    "\n",
    "\n",
    "- [Python3](https://www.python.org/downloads/) (recommend 3.6 or above)\n",
    "- [PyTorch](https://pytorch.org) (recommend 1.0)\n",
    "- [NumPy](http://www.numpy.org) the fundamental package for scientific computing with Python\n",
    "\n",
    "\n",
    "- (Optional) [Anaconda](https://www.anaconda.com/distribution/#download-section), *popular Python Data Science Platform*\n",
    "- (Optional) [TensorFlow](https://www.tensorflow.org/install), but examples are provided as PyTorch\n",
    "- (Optional) [Jupyter](https://jupyter.org/) (Notebook or Lab)\n",
    "- (Optional) [CUDA](https://developer.nvidia.com/cuda-downloads) support GPU\n",
    "\n",
    "\n",
    "Python packages can install by `pip install [package name]` or using **Anaconda** by `conda install [package name]`.\n",
    "\n",
    "*If you are having trouble installing or something else, please contact TA or jiunbae.623@gmail.com.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "First of all, Import some packages for using PyTorch.\n",
    "\n",
    "- torch.nn: The **Network** of PyTorch basically starts with nn.Module.\n",
    "- torch.nn.functional: for **Functions** such as *ReLU*, *MaxPool* (in this example)\n",
    "- torch.optim: for **Optimizers**\n",
    "- torchvision: Handling **Datasets**\n",
    "\n",
    "Numpy the basic scientific computing package used in customary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "PyTorch basically provides MNIST Dataset and support download in running code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '../data' # path to download mnist dataset\n",
    "\n",
    "TRAIN_DATASET = datasets.MNIST(DATASET_DIR,   # Dataset root path\n",
    "                               train=True,    # Train data\n",
    "                               download=True) # Download if not exist\n",
    "\n",
    "TEST_DATASET = datasets.MNIST(DATASET_DIR,    # Dataset root path\n",
    "                              train=False)    # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Visualize MNIST Dataset\n",
    "\n",
    "This code is just show MNIST data (code is not important!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  # PIL is Python Image Library \n",
    "\n",
    "n = 5                  # 5 x 5 matrix\n",
    "size = 28              # Each image is 28 x 28\n",
    "\n",
    "result = Image.new('L', (size * n, size * n))\n",
    "for i, idx in enumerate(np.random.choice(len(TRAIN_DATASET), n * n)):\n",
    "    x = i // n * size\n",
    "    y = i % n * size\n",
    "    result.paste(TRAIN_DATASET[idx][0], (x, y, x + size, y + size))\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Network\n",
    "\n",
    "This is a simple two convolution layer network. The code is quite easy.\n",
    "Just fallow `forward` function. `torch.nn.functional` and `torch.nn` support many useful function and layer for build network.\n",
    "(See offical [doc](https://pytorch.org/docs/stable/nn.html#torch-nn-functional) for more information)\n",
    "\n",
    "`nn.Linear` is linear transformation as `y=ax+b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fully connected](../assets/fullyconnected.jpg)\n",
    "\n",
    "In **PyTorch**, the fc layer can be defined as follow:\n",
    "\n",
    "```\n",
    "nn.Linear(\n",
    "    in_features,\n",
    "    out_features,\n",
    "    bias=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. FC(fully-connected) Model\n",
    "\n",
    "Fill in the blanks to create the following network model.\n",
    "\n",
    "- FC1 (input: 28*28, output: 392)\n",
    "- FC2 (input: 392, output: 10, class number of digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    \"\"\"Simple Neural Network contains fc layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FCNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, ?)\n",
    "        self.fc2 = nn.Linear(?, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single fc step\n",
    "\n",
    "Running a fully connected layer one step is equivalent to doing $wx+b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _ = next(iter(TRAIN_DATASET))\n",
    "image_tensor = torch.Tensor(np.array(image)).reshape(1, -1)\n",
    "\n",
    "layer = nn.Linear(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.forward(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(layer.weight, image_tensor.T).T + layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Network\n",
    "\n",
    "This is a simple two convolution layer network. The code is quite easy.\n",
    "Just fallow `forward` function. `torch.nn.functional` and `torch.nn` support many useful function and layer for build network.\n",
    "(See offical [doc](https://pytorch.org/docs/stable/nn.html#torch-nn-functional) for more information)\n",
    "\n",
    "`nn.Conv2d` is 2-dimentional convolutaion layer.\n",
    "\n",
    "`nn.Linear` is linear transformation as `y=ax+b`.\n",
    "\n",
    "`F.relu` apply ReLU activation function.\n",
    "\n",
    "`F.max_pool2d` apply 2-dimentional max-pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `convolution` layer\n",
    "\n",
    "![convolution layer](../assets/convolution.png)\n",
    "\n",
    "![convolution layer gif](../assets/convolution.gif)\n",
    "\n",
    "## `maxpool` layer\n",
    "\n",
    "![maxpool layer](../assets/maxpool.jpeg)\n",
    "\n",
    "In **PyTorch**, the convolution layer can be defined as follow:\n",
    "\n",
    "```\n",
    "nn.Conv2d(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    "    bias=True,\n",
    "    padding_mode='zeros',\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Convolution Model\n",
    "\n",
    "Fill in the blanks to create the following network model.\n",
    "\n",
    "- Conv1 (input: 1, output: 20, kernel: 5, stride: 1)\n",
    "- ReLU\n",
    "- MaxPool\n",
    "- Conv2 (input: 20, output: 50, kernel: 5, stride: 1)\n",
    "- ReLU\n",
    "- MaxPool\n",
    "- FC1 (input: 4x4x50, output: 500)\n",
    "- ReLU\n",
    "- FC2 (input: 500, output: 10, class number of digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetwork(nn.Module):\n",
    "    \"\"\"Simple Neural Network contains conv layer and fc layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, ?, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(?, ?, 5, 1)\n",
    "        self.fc1 = nn.Linear(?, ?)\n",
    "        self.fc2 = nn.Linear(?, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, ?)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Conv layer output shape\n",
    "\n",
    "We can predict the output shape of the convolution layer by considering variables such as `kernel size`, `stride`, and `padding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, _ = next(iter(TRAIN_DATASET))\n",
    "# change dimension from (28, 28) to (1, 1, 28, 28)\n",
    "# Generally, the input of a machine learning model is (batch_size, channel, width, height).\n",
    "image_tensor = torch.Tensor(np.array(image)).unsqueeze_(0).unsqueeze_(0)\n",
    "\n",
    "layer = nn.Conv2d(1, 20, kernel_size=5, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before executing below the cell, guess output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.forward(image_tensor).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Max pooling layer\n",
    "\n",
    "- Why using maxpooling between convolution layers?\n",
    "- Why using maxpooling? not average-pooling or min-pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(np.array(((1, 2), (3, 4)))).unsqueeze_(0).unsqueeze_(0)\n",
    "F.max_pool2d(x, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Test functions\n",
    "\n",
    "In train scope, we put the data to model, compute loss and update parameters. That's all we have to do.\n",
    "To put data into the model, you call the model as a function that takes data as arguments. (see `output = model(data)`)\n",
    "**PyTorch** provide many loss function. `nll_loss` is the *negative log likelihood loss*. compute loss using output and target(label).\n",
    "Calling `.backward()` accumulates the gradient for each parameters.\n",
    "Finally, call `optimizer.step()` to update optimizer.\n",
    "\n",
    "\n",
    "In test scope, evaluate model using test set which not contained in a train set.\n",
    "`pred = output.argmax(dim=1, keepdim=True)` is change output to digit-class for comparing to target(label).\n",
    "`pred.eq(target.view_as(pred)).sum().item()` calculate correct answer count.\n",
    "\n",
    "\n",
    "`model.train` and `model.test` means set `model` for train or test. In train scope set parameters trainable and test scope not update parameters.\n",
    "So, if you want to update parameters, must call `model.train` before pass data to the model.\n",
    "\n",
    "*`data.to(device)` and `target.to(device)` mean change data to device(gpu if available else cpu)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def train(model, device, train_loader, optimizer) -> float:\n",
    "    \"\"\"Train function\n",
    "\n",
    "        Arguments:\n",
    "            model (nn.Module): some networks extends from ``nn.Module``.\n",
    "            device (torch.device): device for use CUDA if available.\n",
    "            train_loader (torch.utils.data.dataloader.DataLoader): train data loader.\n",
    "            optimizer (torch.optim): optimizer\n",
    "\n",
    "        Returns: loss (float)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move memory to target device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "    \n",
    "        # optimizer initialize\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model, device, test_loader) -> Tuple[float, float, torch.Tensor]:\n",
    "    \"\"\"Test function\n",
    "\n",
    "        Arguments:\n",
    "            model (nn.Module): some networks extends from ``nn.Module``.\n",
    "            device (torch.device): device for use CUDA if available.\n",
    "            test_loader (torch.utils.data.dataloader.DataLoader): test data loader.\n",
    "\n",
    "        Returns: tuple of below three\n",
    "            test_loss: \n",
    "            accuracy: \n",
    "            output: \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, accuracy, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run!\n",
    "\n",
    "\n",
    "### Reproducible (**Important**)\n",
    "\n",
    "**Reproducible** is very **very** ***very*** important in experiment. An experiment that can not be reproduced can not make any conclusions. So fix random seed before anything else.\n",
    "In **PyTorch** just call `torch.manual_seed` for fix random seed. It will set the seed of the random generator, so random results will be **reproducible**.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Unfortunately, machine learning does not mean learning all the variables. We call these parameters the **hyperparameters** that need to be set before learning.\n",
    "In this example, we can set the *learning rate* before training.\n",
    "\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "Loading files from disk is a very expensive operation. Especially in machine learning where a lot of training data is needed, also especially if each data is an image.\n",
    "So, many frameworks provide *data loader* for effectively load data such as use multiple threads and cache. In **PyTorch** DataLoader support shuffle, batch slice, transform and many other functions.\n",
    "But in this example, just use `batch_size` and `shuffle`.\n",
    "\n",
    "PyTorch only process `torch.Tensor`. So, must convert data (3d numpy array) to tensor (torch.Tensor) using transform before training(or test).\n",
    "*`transforms.ToTensor()` automatically transform data to tensor when loader called*\n",
    "\n",
    "\n",
    "### GPU or CPU\n",
    "\n",
    "`device` variable use **CUDA** if available. CPU can get results fast enough because there are fewer data and the network is simple. But when more data is available and the network gets more complicated, it's time to get help from the GPU. So now you do not have to worry.\n",
    "Later `.to (device)` means use the device we specified.\n",
    "\n",
    "\n",
    "### Build Network\n",
    "\n",
    "`model = Network()` create network we defined before. In this example, we use [SGD(Stochastic gradient descent)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # 42, THE ANSWER TO LIFE, THE UNIVERSE AND EVERYTHING\n",
    "\n",
    "batch = 64            # batch size\n",
    "lr = .01              # learning rate\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "TRAIN_DATASET.transform = transforms.ToTensor()\n",
    "train_loader = torch.utils.data.DataLoader(TRAIN_DATASET,\n",
    "                                           batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "TEST_DATASET.transform = transforms.ToTensor()\n",
    "test_loader = torch.utils.data.DataLoader(TEST_DATASET,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "Dataloader provides a batch dataset, and shuffle if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Add Activation\n",
    "\n",
    "Add activation layer in `FCNetwork`.\n",
    "\n",
    "\n",
    "```\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(-1, ?)\n",
    "        x = self.fc1(x)\n",
    "        # add activation\n",
    "        x = self.fc2(x)\n",
    "        # add activation\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "```\n",
    "\n",
    "And change `F.sigmoid` to `F.relu`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNetwork().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer)\n",
    "    test_loss, accuracy, _ = test(model, device, test_loader)\n",
    "    \n",
    "    print('Epoch: {}\\t Loss: {:.6f}'.format(epoch, train_loss))\n",
    "    print('\\t\\t Average Loss: {:.4f}, Accuracy: {:.0f}%'.format(test_loss, accuracy))\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")  # save trained model named 'mnist_cnn.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Visualize test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def show(ary):\n",
    "    display(Image.fromarray(ary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for index, (image, label) in enumerate(TEST_DATASET):\n",
    "    \n",
    "    pred = model(image.to(device).unsqueeze(0))\n",
    "    pred = pred.argmax(dim=1).squeeze()\n",
    "    if label != pred:\n",
    "        show((image.numpy()[0] * 255).astype(np.uint8))    \n",
    "        print(f'Label: {label}, prediction: {pred}')\n",
    "    \n",
    "    if index > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change ConvNetwork Model\n",
    "\n",
    "Compare the experimental results of the FCNetwork model and the ConvNetwork model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNetwork().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer)\n",
    "    test_loss, accuracy, _ = test(model, device, test_loader)\n",
    "    \n",
    "    print('Epoch: {}\\t Loss: {:.6f}'.format(epoch, train_loss))\n",
    "    print('\\t\\t Average Loss: {:.4f}, Accuracy: {:.0f}%'.format(test_loss, accuracy))\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")  # save trained model named 'mnist_cnn.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Change Optimizer\n",
    "\n",
    "The adam optimizer is a better optimizer method that is often used today.\n",
    "\n",
    "![optimizer](../assets/optimizer.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNetwork().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer)\n",
    "    test_loss, accuracy, _ = test(model, device, test_loader)\n",
    "    \n",
    "    print('Epoch: {}\\t Loss: {:.6f}'.format(epoch, train_loss))\n",
    "    print('\\t\\t Average Loss: {:.4f}, Accuracy: {:.0f}%'.format(test_loss, accuracy))\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")  # save trained model named 'mnist_cnn.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
