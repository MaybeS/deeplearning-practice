{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성전자 첨기연 시각 심화\n",
    "\n",
    "- **Instructor**: Jongwoo Lim / Jiun Bae\n",
    "- **Email**: [jlim@hanyang.ac.kr](mailto:jlim@hanyang.ac.kr) / [jiun.maydev@gmail.com](mailto:jiun.maydev@gmail.com)\n",
    "\n",
    "## NeuralNetwork Example\n",
    "\n",
    "In this example you will practice a simple neural network written by only [Numpy](https://www.numpy.org) which is fundamental package for scientific computing with Python. The goals of this example are as follows:\n",
    "\n",
    "- Understand **Neural Networks** and how they work.\n",
    "- Learn basically how to **write and use code**(*Numpy*).\n",
    "\n",
    "*If you are more familiar with PyTorch and TensorFlow(or Keras), You might be wondering why to write from the ground up with numpy instead of the built-in framework. This process is essential for understanding how a neural network works, and if you understand it, will not be too difficult to write in code.*\n",
    "\n",
    "And this example also is written in [IPython Notebook](https://ipython.org/notebook.html), an interactive computational environment, in which you can run code directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments\n",
    "\n",
    "In this assignment, we assume the follows environments. \n",
    "\n",
    "The [Python](https://www.python.org) is a programming language that lets you work quickly and integrate systems more effectively. It is widely used in various fields, and also used in machine learning.\n",
    "\n",
    "The [Pytorch](https://pytorch.org) is an open source deep learning platform, provides a seamless path from research to production.\n",
    "\n",
    "The [Tensorflow](https://www.tensorflow.org) is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\n",
    "\n",
    "The [CUDA®](https://developer.nvidia.com/cuda-zone) Toolkit provides high-performance GPU-accelerated computation. In deep learning, the model takes an age to train without GPU-acceleration. ~~even with the GPU, it still takes a lot of time~~.\n",
    "\n",
    "\n",
    "- [Python3](https://www.python.org/downloads/) (recommend 3.6 or above)\n",
    "- [PyTorch](https://pytorch.org) (recommend 1.0)\n",
    "- [Tensorflow](https://tensorflow.org) (recommend above 1.13.0, but under 2.0 *There are huge difference between 2.0 and below*)\n",
    "- [NumPy](http://www.numpy.org) the fundamental package for scientific computing with Python\n",
    "\n",
    "\n",
    "- (Optional) [Anaconda](https://www.anaconda.com/distribution/#download-section), *popular Python Data Science Platform*\n",
    "- (Optional) [Jupyter](https://jupyter.org/) (Notebook or Lab)\n",
    "- (Optional) [CUDA](https://developer.nvidia.com/cuda-downloads) support GPU\n",
    "\n",
    "\n",
    "Python packages can install by `pip install [package name]` or using **Anaconda** by `conda install [package name]`.\n",
    "\n",
    "*If you are having trouble installing or something else, please contact TA or jiun.maydev@gmail.com.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "Numpy the basic scientific computing package used in customary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "PyTorch basically provides MNIST Dataset and support download in running code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './data' # path to download mnist dataset\n",
    "\n",
    "TRAIN_DATASET = datasets.MNIST(DATASET_DIR,   # Dataset root path\n",
    "                               train=True,    # Train data\n",
    "                               download=True) # Download if not exist\n",
    "\n",
    "TEST_DATASET = datasets.MNIST(DATASET_DIR,    # Dataset root path\n",
    "                              train=False)    # Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "This is a simple two dense(fully connected) layer network. The code is quite easy.\n",
    "\n",
    "So, whole network architecture as follow:\n",
    "\n",
    "- Dense\n",
    "- ReLU\n",
    "- Dense\n",
    "- ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    pass\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        self.weights = np.random.randn(input_units, output_units) * .01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        return np.dot(inputs, self.weights) + self.biases\n",
    "      \n",
    "    def backward(self, grads):\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grads, np.transpose(self.weights))\n",
    "\n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        self.grad_weights = np.transpose(np.dot(np.transpose(grads), self.inputs))\n",
    "        self.grad_biases = np.sum(grads, axis = 0)\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "    def update(self, lr: float = .01):\n",
    "        # Here we perform a stochastic gradient descent step.\n",
    "        self.weights = self.weights - lr * self.grad_weights\n",
    "        self.biases = self.biases - lr * self.grad_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return 1. / (1. + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, grads):\n",
    "        \n",
    "        r = self.forward(self.inputs)\n",
    "        return grads * r * (1. - r)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    return -preds[np.arange(len(preds)), labels] + np.log(np.sum(np.exp(preds),axis=-1))\n",
    "\n",
    "def grad_fn(preds, labels):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(preds)\n",
    "    ones_for_answers[np.arange(len(preds)), labels] = 1\n",
    "    \n",
    "    softmax = np.exp(preds) / np.exp(preds).sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def train(networks: List[Layer], X, y):\n",
    "    preds = reduce(lambda inputs, layer: layer.forward(inputs), [X, *networks])\n",
    "    \n",
    "    loss = loss_fn(preds, y)\n",
    "    grads = grad_fn(preds, y)\n",
    "    \n",
    "    grads = reduce(lambda grads, layer: layer.backward(grads), [grads, *reversed(networks)])\n",
    "\n",
    "    for layer in networks:\n",
    "        layer.update(lr)\n",
    "    \n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch):\n",
    "    for b in range(int(len(dataset) / batch)):\n",
    "        images = np.empty((batch, 28, 28), dtype=np.float32)\n",
    "        labels = np.empty(batch, dtype=np.uint8)\n",
    "        \n",
    "        for i in range(batch):\n",
    "            images[i], labels[i] = dataset[b * batch + i]\n",
    "        \n",
    "        images = np.reshape(images / 255., (batch, -1))\n",
    "        \n",
    "        yield images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .3\n",
    "batch = 128\n",
    "epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = [\n",
    "    Dense(28*28, 100),\n",
    "    Sigmoid(),\n",
    "    Dense(100, 200),\n",
    "    Sigmoid(),\n",
    "    Dense(200, 10),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train scope\n",
    "    train_loss, test_loss, test_acc = 0, 0, 0\n",
    "    for images, labels in get_batch(TRAIN_DATASET, batch):\n",
    "        train_loss += train(networks, images, labels)\n",
    "    \n",
    "    for images, labels in get_batch(TEST_DATASET, batch):\n",
    "        preds = reduce(lambda inputs, layer: layer.forward(inputs), [images, *networks])\n",
    "\n",
    "        test_loss += loss_fn(preds, labels).mean()\n",
    "        test_acc += (preds.argmax(axis=-1) == labels).mean()\n",
    "    \n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\tTrain Loss: {train_loss / (len(TRAIN_DATASET) / batch)}')\n",
    "    print(f'\\tTest Loss: {test_loss / (len(TEST_DATASET) / batch)}')\n",
    "    print(f'\\tTest Acc: {test_acc / (len(TEST_DATASET) / batch)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = random.choice(TEST_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(lambda inputs, layer: layer.forward(inputs), [np.reshape(np.array(image), -1)[None, :], *networks]).argmax(axis=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
